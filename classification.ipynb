{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6dac65d-3e4b-4da8-b730-5126d271f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c074ca6d-0382-47ee-b03a-dc8caed6e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:5\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e84557-6380-4d0a-abc4-d9f39e5c5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0670a9-f70e-408c-bb4f-57ab43be522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e44663f0-6431-4597-b8f3-000d296151c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/train_data\"\n",
    "test_dir = \"data/test_data\"\n",
    "no_dir  = \"synthetic no finding\"\n",
    "inf_dir = \"synthetic infiltration\"\n",
    "csv_path = \"final data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1e11818-96f1-4f0c-aa68-1fbe654f5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f16f4421-b574-40f3-85e4-95ab775704d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames_and_labels(df, folder):\n",
    "    filenames = df['Image Index'].tolist()\n",
    "    labels = df['Label'].tolist()\n",
    "    paths = [os.path.join(folder, fname) for fname in filenames]\n",
    "    return paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2450853b-adac-40a1-aa4f-8e0692db845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, train_labels = get_filenames_and_labels(df_labels[df_labels['Image Index'].isin(os.listdir(train_dir))], train_dir)\n",
    "test_paths, test_labels = get_filenames_and_labels(df_labels[df_labels['Image Index'].isin(os.listdir(test_dir))], test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a6f3e3a-e9af-4ddf-9186-7c88c79b5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ChestXrayDataset(train_paths, train_labels, transform=train_transform)\n",
    "test_ds  = ChestXrayDataset(test_paths,  test_labels,  transform=train_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,  num_workers=4)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8525d32d-8652-40b1-a1de-22b9d04d1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_no_paths  = glob.glob(os.path.join(no_dir,  \"*.*\"))\n",
    "extra_inf_paths = glob.glob(os.path.join(inf_dir, \"*.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ed998d1-5b34-4234-8e92-1709480630a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_no_labels  = [0] * len(extra_no_paths)\n",
    "extra_inf_labels = [1] * len(extra_inf_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2ce99-e5e9-4c13-98c3-dc06d76265dc",
   "metadata": {},
   "source": [
    "# Training on Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68467632-8fbf-4a5d-bcab-402ac79b1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "val_scores = []\n",
    "best_val_acc = 0\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8bfaf12-e676-46df-8daf-bcfd87624a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spring2024/sc9422/miniconda3/envs/idai780/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/spring2024/sc9422/miniconda3/envs/idai780/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — train_acc: 0.6888, val_acc: 0.7049\n",
      "Epoch 2 — train_acc: 0.7136, val_acc: 0.7276\n",
      "Epoch 3 — train_acc: 0.7289, val_acc: 0.7227\n",
      "Epoch 4 — train_acc: 0.7439, val_acc: 0.7260\n",
      "Epoch 5 — train_acc: 0.7575, val_acc: 0.7115\n",
      "Epoch 6 — train_acc: 0.7816, val_acc: 0.7069\n",
      "Epoch 7 — train_acc: 0.8101, val_acc: 0.7140\n",
      "Epoch 8 — train_acc: 0.8399, val_acc: 0.7189\n",
      "Epoch 9 — train_acc: 0.8687, val_acc: 0.7032\n",
      "Epoch 10 — train_acc: 0.8985, val_acc: 0.7005\n",
      "\n",
      "Fold 2\n",
      "Epoch 1 — train_acc: 0.6842, val_acc: 0.7071\n",
      "Epoch 2 — train_acc: 0.7151, val_acc: 0.7250\n",
      "Epoch 3 — train_acc: 0.7250, val_acc: 0.7324\n",
      "Epoch 4 — train_acc: 0.7430, val_acc: 0.7250\n",
      "Epoch 5 — train_acc: 0.7640, val_acc: 0.7081\n",
      "Epoch 6 — train_acc: 0.7853, val_acc: 0.7124\n",
      "Epoch 7 — train_acc: 0.8089, val_acc: 0.6991\n",
      "Epoch 8 — train_acc: 0.8403, val_acc: 0.7150\n",
      "Epoch 9 — train_acc: 0.8738, val_acc: 0.7168\n",
      "Epoch 10 — train_acc: 0.9000, val_acc: 0.6955\n",
      "\n",
      "Fold 3\n",
      "Epoch 1 — train_acc: 0.6907, val_acc: 0.7197\n",
      "Epoch 2 — train_acc: 0.7192, val_acc: 0.6961\n",
      "Epoch 3 — train_acc: 0.7304, val_acc: 0.7266\n",
      "Epoch 4 — train_acc: 0.7434, val_acc: 0.7161\n",
      "Epoch 5 — train_acc: 0.7590, val_acc: 0.7163\n",
      "Epoch 6 — train_acc: 0.7762, val_acc: 0.7214\n",
      "Epoch 7 — train_acc: 0.8124, val_acc: 0.6994\n",
      "Epoch 8 — train_acc: 0.8452, val_acc: 0.6854\n",
      "Epoch 9 — train_acc: 0.8758, val_acc: 0.6881\n",
      "Epoch 10 — train_acc: 0.8965, val_acc: 0.6957\n",
      "\n",
      "Fold 4\n",
      "Epoch 1 — train_acc: 0.6886, val_acc: 0.7224\n",
      "Epoch 2 — train_acc: 0.7168, val_acc: 0.7155\n",
      "Epoch 3 — train_acc: 0.7300, val_acc: 0.6880\n",
      "Epoch 4 — train_acc: 0.7419, val_acc: 0.7212\n",
      "Epoch 5 — train_acc: 0.7570, val_acc: 0.7235\n",
      "Epoch 6 — train_acc: 0.7780, val_acc: 0.7230\n",
      "Epoch 7 — train_acc: 0.8072, val_acc: 0.6905\n",
      "Epoch 8 — train_acc: 0.8385, val_acc: 0.7136\n",
      "Epoch 9 — train_acc: 0.8645, val_acc: 0.7022\n",
      "Epoch 10 — train_acc: 0.8926, val_acc: 0.7061\n",
      "\n",
      "Fold 5\n",
      "Epoch 1 — train_acc: 0.6903, val_acc: 0.7120\n",
      "Epoch 2 — train_acc: 0.7145, val_acc: 0.7129\n",
      "Epoch 3 — train_acc: 0.7285, val_acc: 0.7258\n",
      "Epoch 4 — train_acc: 0.7444, val_acc: 0.7054\n",
      "Epoch 5 — train_acc: 0.7590, val_acc: 0.7104\n",
      "Epoch 6 — train_acc: 0.7804, val_acc: 0.7118\n",
      "Epoch 7 — train_acc: 0.8099, val_acc: 0.7003\n",
      "Epoch 8 — train_acc: 0.8391, val_acc: 0.7079\n",
      "Epoch 9 — train_acc: 0.8675, val_acc: 0.6935\n",
      "Epoch 10 — train_acc: 0.8939, val_acc: 0.6862\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_paths)):\n",
    "    print(f\"\\nFold {fold+1}\")\n",
    "    fold_train_paths = [train_paths[i] for i in train_idx]\n",
    "    fold_train_labels = [train_labels[i] for i in train_idx]\n",
    "    fold_val_paths = [train_paths[i] for i in val_idx]\n",
    "    fold_val_labels = [train_labels[i] for i in val_idx]\n",
    "\n",
    "    train_ds = ChestXrayDataset(fold_train_paths, fold_train_labels, transform=train_transform)\n",
    "    val_ds = ChestXrayDataset(fold_val_paths, fold_val_labels, transform=train_transform)\n",
    "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = torchvision.models.resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        running_corrects = 0\n",
    "        for imgs, labs in train_loader:\n",
    "            imgs, labs = imgs.to(device), labs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_corrects += (outputs.argmax(1) == labs).sum().item()\n",
    "        train_acc = running_corrects / len(train_ds)\n",
    "\n",
    "        model.eval()\n",
    "        val_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labs in val_loader:\n",
    "                imgs, labs = imgs.to(device), labs.to(device)\n",
    "                outputs = model(imgs)\n",
    "                val_corrects += (outputs.argmax(1) == labs).sum().item()\n",
    "        val_acc = val_corrects / len(val_ds)\n",
    "        print(f\"Epoch {epoch+1} — train_acc: {train_acc:.4f}, val_acc: {val_acc:.4f}\")\n",
    "\n",
    "    val_scores.append(val_acc)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e524a68-fec3-484b-8512-38add5169738",
   "metadata": {},
   "source": [
    "Testing on unseen data with real images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f900d14-8092-4d3f-a0d1-991406a98641",
   "metadata": {},
   "source": [
    "# Training on Original + Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d033506-128c-4bc4-ab02-97dfaa54eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_paths = train_paths + extra_no_paths + extra_inf_paths\n",
    "combined_labels = train_labels + extra_no_labels + extra_inf_labels\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "val_scores_syn = []\n",
    "best_val_acc_syn = 0\n",
    "best_model_syn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675350a-c33a-45e7-9325-fad959f005ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Epoch 1 — train_acc: 0.8610, val_acc: 0.8892\n",
      "Epoch 2 — train_acc: 0.8902, val_acc: 0.8442\n",
      "Epoch 3 — train_acc: 0.9137, val_acc: 0.8619\n",
      "Epoch 4 — train_acc: 0.9336, val_acc: 0.8733\n",
      "Epoch 5 — train_acc: 0.9450, val_acc: 0.8575\n",
      "Epoch 6 — train_acc: 0.9522, val_acc: 0.8430\n",
      "Epoch 7 — train_acc: 0.9577, val_acc: 0.8462\n",
      "Epoch 8 — train_acc: 0.9639, val_acc: 0.8327\n",
      "Epoch 9 — train_acc: 0.9670, val_acc: 0.8332\n",
      "Epoch 10 — train_acc: 0.9674, val_acc: 0.8361\n",
      "\n",
      "Fold 2\n",
      "Epoch 1 — train_acc: 0.9353, val_acc: 0.9651\n",
      "Epoch 2 — train_acc: 0.9540, val_acc: 0.9665\n",
      "Epoch 3 — train_acc: 0.9642, val_acc: 0.9468\n",
      "Epoch 4 — train_acc: 0.9682, val_acc: 0.9279\n",
      "Epoch 5 — train_acc: 0.9725, val_acc: 0.9488\n",
      "Epoch 6 — train_acc: 0.9726, val_acc: 0.9459\n",
      "Epoch 7 — train_acc: 0.9754, val_acc: 0.9419\n",
      "Epoch 8 — train_acc: 0.9786, val_acc: 0.9287\n",
      "Epoch 9 — train_acc: 0.9768, val_acc: 0.9360\n",
      "Epoch 10 — train_acc: 0.9764, val_acc: 0.9160\n",
      "\n",
      "Fold 3\n",
      "Epoch 1 — train_acc: 0.9661, val_acc: 0.9883\n",
      "Epoch 2 — train_acc: 0.9741, val_acc: 0.9761\n",
      "Epoch 3 — train_acc: 0.9764, val_acc: 0.9741\n",
      "Epoch 4 — train_acc: 0.9791, val_acc: 0.9777\n",
      "Epoch 5 — train_acc: 0.9811, val_acc: 0.9605\n",
      "Epoch 6 — train_acc: 0.9822, val_acc: 0.9731\n",
      "Epoch 7 — train_acc: 0.9841, val_acc: 0.9617\n",
      "Epoch 8 — train_acc: 0.9824, val_acc: 0.9637\n",
      "Epoch 9 — train_acc: 0.9833, val_acc: 0.9500\n",
      "Epoch 10 — train_acc: 0.9849, val_acc: 0.9564\n",
      "\n",
      "Fold 4\n",
      "Epoch 1 — train_acc: 0.9776, val_acc: 0.9865\n",
      "Epoch 2 — train_acc: 0.9822, val_acc: 0.9771\n",
      "Epoch 3 — train_acc: 0.9830, val_acc: 0.9647\n",
      "Epoch 4 — train_acc: 0.9837, val_acc: 0.9752\n",
      "Epoch 5 — train_acc: 0.9856, val_acc: 0.9764\n",
      "Epoch 6 — train_acc: 0.9875, val_acc: 0.9725\n",
      "Epoch 7 — train_acc: 0.9870, val_acc: 0.9706\n",
      "Epoch 8 — train_acc: 0.9862, val_acc: 0.9660\n",
      "Epoch 9 — train_acc: 0.9875, val_acc: 0.9568\n",
      "Epoch 10 — train_acc: 0.9863, val_acc: 0.9676\n",
      "\n",
      "Fold 5\n",
      "Epoch 1 — train_acc: 0.9806, val_acc: 0.9938\n",
      "Epoch 2 — train_acc: 0.9850, val_acc: 0.9915\n",
      "Epoch 3 — train_acc: 0.9856, val_acc: 0.9832\n",
      "Epoch 4 — train_acc: 0.9865, val_acc: 0.9805\n",
      "Epoch 5 — train_acc: 0.9870, val_acc: 0.9784\n",
      "Epoch 6 — train_acc: 0.9884, val_acc: 0.9800\n",
      "Epoch 7 — train_acc: 0.9899, val_acc: 0.9764\n",
      "Epoch 8 — train_acc: 0.9880, val_acc: 0.9739\n",
      "Epoch 9 — train_acc: 0.9889, val_acc: 0.9748\n",
      "Epoch 10 — train_acc: 0.9896, val_acc: 0.9670\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(combined_paths)):\n",
    "    print(f\"\\nFold {fold+1}\")\n",
    "    fold_train_paths = [combined_paths[i] for i in train_idx]\n",
    "    fold_train_labels = [combined_labels[i] for i in train_idx]\n",
    "    fold_val_paths = [combined_paths[i] for i in val_idx]\n",
    "    fold_val_labels = [combined_labels[i] for i in val_idx]\n",
    "\n",
    "    train_ds = ChestXrayDataset(fold_train_paths, fold_train_labels, transform=train_transform)\n",
    "    val_ds = ChestXrayDataset(fold_val_paths, fold_val_labels, transform=train_transform)\n",
    "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    model_syn = torchvision.models.resnet18(pretrained=True)\n",
    "    model_syn.fc = nn.Linear(model_syn.fc.in_features, 2)\n",
    "    model_syn = model_syn.to(device)\n",
    "    optimizer = optim.Adam(model_syn.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model_syn.train()\n",
    "        running_corrects = 0\n",
    "        for imgs, labs in train_loader:\n",
    "            imgs, labs = imgs.to(device), labs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_syn(imgs)\n",
    "            loss = criterion(outputs, labs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_corrects += (outputs.argmax(1) == labs).sum().item()\n",
    "        train_acc = running_corrects / len(train_ds)\n",
    "\n",
    "        model_syn.eval()\n",
    "        val_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labs in val_loader:\n",
    "                imgs, labs = imgs.to(device), labs.to(device)\n",
    "                outputs = model_syn(imgs)\n",
    "                val_corrects += (outputs.argmax(1) == labs).sum().item()\n",
    "        val_acc = val_corrects / len(val_ds)\n",
    "        print(f\"Epoch {epoch+1} — train_acc: {train_acc:.4f}, val_acc: {val_acc:.4f}\")\n",
    "\n",
    "    val_scores_syn.append(val_acc)\n",
    "    if val_acc > best_val_acc_syn:\n",
    "        best_val_acc_syn = val_acc\n",
    "        best_model_syn = copy.deepcopy(model_syn.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f3b50d-86da-4538-98e7-1b6ff3b3ff4f",
   "metadata": {},
   "source": [
    "# Testing on completely unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b5839-ca69-47b1-8d5e-2a30b309196e",
   "metadata": {},
   "source": [
    "The cross validation above also included the synthetic data. Hence, the next step is testing both the models' performance using unseen test data that was set aside and not even used during the GAN's training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dabf6fe1-c9d7-423f-97a8-d5b96d88cf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Original): 0.7046\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "test_corrects = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labs in test_loader:\n",
    "        imgs, labs = imgs.to(device), labs.to(device)\n",
    "        outputs = model(imgs)\n",
    "        test_corrects += (outputs.argmax(1) == labs).sum().item()\n",
    "test_acc = test_corrects / len(test_loader.dataset)\n",
    "print(f\"Test Accuracy (Original): {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0c2c1b0-6804-4c4e-b4e4-ece4b2fb52bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Original+synthetic): 0.7224\n"
     ]
    }
   ],
   "source": [
    "model_syn.load_state_dict(best_model_syn)\n",
    "model_syn.eval()\n",
    "test_corrects = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labs in test_loader:\n",
    "        imgs, labs = imgs.to(device), labs.to(device)\n",
    "        outputs = model_syn(imgs)\n",
    "        test_corrects += (outputs.argmax(1) == labs).sum().item()\n",
    "test_acc = test_corrects / len(test_loader.dataset)\n",
    "print(f\"Test Accuracy (Original+synthetic): {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23dd996-854b-4223-a938-83814791e396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
